# Template Code Generator Fine-Tuning Dataset

This folder holds artefacts for training a supervised fine-tuned model that emits Remotion code from natural-language briefs. The dataset pairs realistic prompts (synthesized from canonical metadata) with the exact TSX code served in production templates.

## Layout
- `overrides.json` *(optional)* – manual prompt additions or tweaks (script will merge them with autogenerated prompts).
- `v1/` – generated outputs (`train.jsonl`, `validation.jsonl`, `test.jsonl`, plus `stats.json`).

The dataset is generated via `scripts/generate-template-code-sft.ts`, which pulls canonical metadata from `src/templates/metadata/canonical.ts` and DB template code from `public."bazaar-vid_templates".tsx_code`.

## Usage
```bash
npm run data:code-sft -- --dry-run      # preview a handful of examples
npm run data:code-sft                  # build v1/train.jsonl, v1/validation.jsonl, v1/test.jsonl
```

Each JSONL entry follows OpenAI's chat fine-tune schema: minimal system instruction, a natural-language brief that mentions format + duration, and the TSX code response. The generator enforces format gating, deterministic splits (seeded), and optional metadata for QA.

Set `DATABASE_URL` before running so the script can pull the latest TSX code for DB templates. For local/staging experimentation, point the env var at the development database.
