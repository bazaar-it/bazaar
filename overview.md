what are we building. what are we doing. and why. 

## ðŸŽ¯ WHAT IS BAZAAR-VID?

Bazaar is a **sophisticated AI-powered video creation platform** that enables users to generate custom video content (motion graphics) through natural language prompts or uploaded imagages. We soon aim to add the oppertunity for users to connect third party services, that will work as context for our system to make even better videos for them. For exmaple, letting users connect their Figma or Github repository, and we can scrape all their config files, we can scrape the styling of their homepage, and alredy gather a lot of that our system can leverage to make even more custom-specifig videos for the specific users. The videos are generteded pureley by React/remotion code, generated by Large Language models.

Currently, our system does not support anything else that pure react code. No other imports of any libraries. No Audio. No images. No videos. 
Currenlty, the codebase is extremnely messy. is sprint 40-42 (memory-bank/sprints/sprint40, memory-bank/sprints/sprint41,memory-bank/sprints/sprint42 ), we have been trying to clean up the reposioty, because there are so manu ophaned files, that are not used anymor - because we have been growing the engineer team and the new guys have trouble manouvering in the codebase. not just from the oprahned files, but also from the insane degree of techical depth. Even tho our system is quite "simple", most of the code in this codebase have been sporadically written by different LLM models, without the awereness of what we are trying to do , and where we are going. But its been un-successful because of lack of focus. we have been trying to clean the codebase, at the same time as developing new features. and trying to modularixe files that previously has been 2500 lines long into 100-300 line files.

One of the biggest reasons we have been fialing so far is that the team itself dont relly understand how this works. It "works" purealy by luck. Our live users are able to uplaod screenshots, ireatre on prompt, and some way or another, by some miracle, our system generetd some code, (it usually takes 2 minutes from user input in the Chatpanel to the AI-generated code is provided back to the user (!!)). Bur the problem is - no one on the team is able to, with certaintly, to point to which system prompt is actually being used. we have 40+ system prompts inside the codebase, and we dont know which one is actually being used. - that is critial, because how can we develop our prodcut at bazaar.it, if we dont even know ourselves how it works. 

Lets go thru this in detail. We aim to develop a system that is intelligent and agentic. such that, we dont really focus on building a UI, but rather the tools, such that other agents can communicate with our system to generete great videos. Lets say, you are a software company, you are launhing a new feature, and you want to communicate this to your followers, or in a newsletter, or internally to the team - you should be able to "tag" bazaar in a PR in github (or just set up an automation, or tell ChatGPT or Cursor to make a video on it - and these other AI agents are communicating with Our system , because we have the mcp tools that can be used for this. we have all the best tools to create great motion grphics. - but in order to get there, we do think we need to start with having a UI. So now, we have a UI, with a simple chatpanel (which works from the primary input/output - users describe what they want, if they want to edit a scene, add a scene, delete a scene, change duratio, uplaod image, etc etc,), and we have a previewpanel - which is a Remotion video.
 (Remotion is an open-source framework that lets you write a video the same way you build a React pageâ€”then export it as a real MP4 or WebM file. In other words, instead of key-framing in a timeline editor, you describe each frame with JSX, pass data in as props, and let the tool render the final clip.  ï¿¼
)

- So at the moment, we have designed a brain orcestrator, wher ewe initlize a llm which respoinble for choosing the correct tool/tools - based on the user input and the context of the current user input
- The tools is AddScene, EditScene or DeleteScene.
- Add scene can take both 1 pure text input or 2 image (and text)
    - text description as input (Animate a video for Finance.ai. Use white text on a black background. Add a heading that says 'Smarter Finance. Powered by AI.' The subheading is 'Automate reports, optimize decisions, and forecast in real-time.' Use blue and white colors similar to Facebook's branding. At the bottom center, add a neon blue 'Try Now' button with a gentle pulsing animation.)-- 
- Add scene can either be the first inital scene the users is making, or it can be an additional scene, Scene 1, Scene x. if a user is creating their first scene - aka the storyboardSoFar is empty - we need a way to tell our system what standards to achieve. with some trial and error we found a 2 step process which consist of first initilizaing a LLM to generate a highly detailed layout Json from the user input, with colors, UI compnents, animaitons, text faed, backgorund color, etc etc - and the second process is taking that layout json, tkaing the user pormpt, and generating  remotion-compatible JSX code that is rendered real time in the remotion player. I dont think this is optimal, now it ususally takes 75 seconds. and the results are so-so. its not very good. If a users wants a addscene, and the srotybaord is not empty aka scene 1 alreay esxists Â´, we dont need to go thru the layout json again, just use the layout from scene 1 (or all the previous scene, a nd make the new scene match the style layout)
- if a user uplaods image, we dont want to go thru the two-step process. then we just feed the image direlcty into our AI-code generetor - which is system pormpted to look at the image and generete a animation JSX that represents that image - and uses the user prompt as input. if a user uplaods a image and pormpt - make this but change the text to "X, and make the circes be triangles and colors to be blue - our AI-codegen should generete a motion grpahic video, based on the image, but considering the uer prompt
- 
- Edit scene, should obvisoalu take the context into consideration, and the suer prompt, with out without image. so if a suer says "change bacjground volor to blue", it should takew the exsiting code, change background color to blue. or if a suer uplaods a image and says "make the button look like this" it should change look at the image, look at the code, understand that there is a button animatin in the code for that sceen, and change the jsx code that represents the button to match the uplaoded screenshot of a button and return that code, so the user can see the new video in real time. 
- we use Sonnet 4 for all codegen - so its a highly capable model, multomodal model, so it can take image as inut , we dont need  speerate AI  - analsye image - then codegen.

But there are many things that we havent really gigured out. 
1 - how to optimally use context. what is context? whos should provide context? we do save a lot of data from eahc user iteration. we save each user prompt, each respoince each iteration of the code the brain reasning of which tools that were used, and what the message from the rbain to the tools were. but we dont relaly know how to design this sytem best way possible. what is the way its done? industry dtanrda of new AI startups using mcp tools. what is a brain? should the brain only choose tools? should the brina first GetContext - then choose tool/tools? and send the required context into each tool? the 3 tools have different system services they used, depeding on what verison of the tool is being used. addscene with image. add scene without storyboaurd. edit - surgical (small edit like make the button blue), edit - creative (make better) --- with our without image. i keep getting a feeling that we might have overcoimplicated this.... that these models are actually betetr than we assume them to be. that they understand more than we might think. tht we might have optiimaed our system for the wrongs things... We should assume that the models are just becoming better and better, more and more cpaable, faster and faster. and we should be focusing on adding more nad more features, like Veo 3 from Google, sounds from elevenlabs. just finding a basic system that works, not reinvting the wheel in terms of and AI understanding intent and choosing tools. shoudl the brain just GetIntent - how? based on the entire chat history? how are there chatGPT or claude code doing it? how do they choose the correct tools? and do the brain provide the context toi the tools, or shoudl the brain just choose tools, and then the tools themselves query the context? and how many different system pormpt do we need? and should alls tools have subtools? or should the tools have different services? how many servcies should eahc tool have? each tool should not initialixe its own agent , or should it? and what baout multistep tools? when a user says, delete the button from scene 2 and add it into a new scene. and how do we make sure the codegen is genereted good detaield code? that actually makes great motoin graphics videos? and how long time should this take? now everything takes 2 minutes. that has to mean that we are doing something suboptimal in our system it sohulod not be that slow should it? 

What si the best way to deal with context? 
What is the best way of dealing with intent?
What is the best way of dealing with tools? and  how much reposinbliity should the tools have? and should the brain jut schoose tools and then close? or should the rbain itself wait for the response from the tools? or should just the generationts - aka the api layer that calls the brain, be respoinble for executing the tools tht is being chosen by the brain?### Tech Stack:
- **Frontend**: Next.js 15 + React 19 + Tailwind CSS + shadcn/ui
- **Backend**: tRPC v11 + Next.js API Routes  
- **Database**: PostgreSQL (Neon) + Drizzle ORM
- **Storage**: Cloudflare R2 for assets
- **Video**: Remotion for composition and rendering
- **AI**: OpenAI GPT-4o-mini with multi-agent system, Claude sonnet 4 for codegen. 
- **Real-time**: Server-Sent Events + JSON Patch for collaboration
